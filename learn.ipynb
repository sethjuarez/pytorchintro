{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Digit Recognition Task\n","If this is your first time running a notebook - welcome!! Notebooks are awesome because they let us play around and experiment\n","with code with near-instant feedback. Some pointers:\n","1. To execute a cell, click on it and hit SHIFT-Enter\n","2. Once something is executed, the variables are in memory - inspect them!\n","\n","## Getting Started\n","This first cell imports the necessary libraries so we can get started:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","import numpy as np\n","from PIL import Image\n","import torch.nn as nn\n","import torch.onnx as onnx\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms"]},{"cell_type":"markdown","metadata":{},"source":["## Data!\n"," Without data we really can't do anything with machine learning. At this point we have our sharp question **can we predict a digit given a 28x28 vector of numbers?**\n"," \n"," Once that is all squared away we need to take a look at our data. The next cell is a helper function that visualizes the the digits (a sanity check).\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def draw_digits(digits):\n","    fig, axes = plt.subplots(6, 20, figsize=(18, 7),\n","                            subplot_kw={'xticks':[], 'yticks':[]},\n","                            gridspec_kw=dict(hspace=0.1, wspace=0.1))\n","    for i, ax in enumerate(axes.flat):\n","        X, y = digits[i]\n","        ax.imshow(255 - X.reshape(28,28) * 255, cmap='gray')\n","        ax.set_title('{:.0f}'.format(torch.argmax(y).item()))"]},{"cell_type":"markdown","metadata":{},"source":["The next cell downloads the standard digit dataset (called MNIST). The `transform` and `target_transform` parts of this call add some conversion steps to make the data more suitable for the models we will try."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["digits = datasets.MNIST('data', train=True, download=True,\n","                        transform=transforms.Compose([\n","                            transforms.ToTensor(),\n","                            transforms.Lambda(lambda x: x.reshape(28*28))\n","                        ]),\n","                        target_transform=transforms.Compose([\n","                            transforms.Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n","                        ])\n","                     )"]},{"cell_type":"markdown","metadata":{},"source":["Here is our sanity check!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["draw_digits(digits)"]},{"cell_type":"markdown","metadata":{},"source":["Feel free to take a look at the data by printing out `x` and/or `y`. They really are just numbers! A couple of things that might seem strange when you print them out:\n","1. \"I thought you said the images were 784 sized vectors of 0-255???\" - They were! We just normalized the vectors by dividing by 255 so the new range is 0-1 (it makes the numbers more tidy)\n","2. \"I thought the `y` was a numerical answer?? Instead it's a 10 sized vector!\" - Yes - this is called a one-hot encoding of the answer. Now there's a `1` in the index of the right answer. Again, this makes the math work a lot better for the models we will be creating."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x, y = digits[0]\n","print(x)\n","print(y)"]},{"cell_type":"markdown","metadata":{},"source":["# Choosing Models\n","Now that we have some data it's time to start picking models we think might work. This is where the science part of data-science comes in: we guess and then check if our assumptions were right. Imagine models like water pipes that have to distribute water to 10 different hoses depending on 784 knobs. These 784 knobs represent the individual pixels in the digit and the 10 hoses at the end represent the actual number (or at least the index of the one with the most water coming out of it). Our job now is to pick the plumbing in between.\n","\n","The next three cells represent three different constructions in an increasingly more complex order:\n","\n","1. The first is a simple linear model,\n","2. The second is a 3 layer Neural Network,\n","3. and the last is a full convolutional neural network\n","\n","While it is out of the scope of this tutorial to fully explain how they work, just imagine they are basically plumbing with internal knobs that have to be tuned to produce the right water pressure at the end to push the most water out of the right\n","index. As you go down each cell the plumbing and corresponding internal knobs just get more complicated."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class SimpleLinear(nn.Module):\n","    def __init__(self):\n","        super(SimpleLinear, self).__init__()\n","        self.layer1 = nn.Linear(28*28, 10)\n","\n","    def forward(self, x):\n","        x = self.layer1(x)\n","        return F.softmax(x, dim=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class NeuralNework(nn.Module):\n","    def __init__(self):\n","        super(NeuralNework, self).__init__()\n","        self.layer1 = nn.Linear(28*28, 512)\n","        self.layer2 = nn.Linear(512, 512)\n","        self.output = nn.Linear(512, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.layer1(x))\n","        x = F.relu(self.layer2(x))\n","        x = self.output(x)\n","        return F.softmax(x, dim=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class CNN(nn.Module):\n","    def __init__(self):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n","        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n","        self.conv2_drop = nn.Dropout2d()\n","        self.fc1 = nn.Linear(320, 50)\n","        self.fc2 = nn.Linear(50, 10)\n","\n","    def forward(self, x):\n","        x = x.view(-1, 1, 28, 28)\n","        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n","        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n","        x = x.view(-1, 320)\n","        x = F.relu(self.fc1(x))\n","        x = F.dropout(x, training=self.training)\n","        x = self.fc2(x)\n","        return F.softmax(x, dim=1)"]},{"cell_type":"markdown","metadata":{},"source":["# Optimizing Model Parameters\n","Now that we have some models it's time to optimize the internal parameters to see if it can do a good job at recognizing digits! It turns out there are some parameters that we can give the optimization algorithm to tune how it trains - these are called hyper-parameters. That's what the two variables represent below:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["learning_rate = 1e-3\n","batch_size = 64\n","epochs = 5"]},{"cell_type":"markdown","metadata":{},"source":["The `learning_rate` basically specifies how fast the algorithm will learn the model parameters. Right now you're probably thinking \"let's set it to fifty million #amirite?\" The best analogy for why this is a bad idea is golf. I'm a terrible golfist (is that right?) so I don't really know anything - but pretend you are trying to sink a shot (again sorry) but can only hit the ball the same distance every time. Easy right? Hit it the exact length from where you are to the hole! Done! Now pretend you don't know where the hole is but just know the general direction. Now the distance you choose actually matters. If it is too long a distance you'll miss the hole, and then when you hit it back you'll overshoot again. If the distance is too small then it will take forever to get there but for sure you'll eventually get it in. Basically you have to guess what the right distance per shot should be and then try it out. That is basically what the learning rate does for finding the \"hole in one\" for the right parameters (ok, I'm done with the golf stuff).\n","\n","Below there are three things that make this all work:\n","1. **The Model** - this is the function we're making that takes in the digit vector and should return the right number\n","2. **The Cost Function** (sometimes called the loss function). I know I promised I was done with golf but I lied. Remember how I said in our screwy golf game you knew the general direction of the hole? The cost function tells us the distance to the hole - when it's zero we're there! In actual scientific terms, the cost function tells us how bad the model is at getting the right answer. As we take shots you should see the cost function decreasing. If this does not happen then something is wrong. At this point I would change the shot distance (or `learning_rate`) to something smaller and try again. If that doesn't work maybe change the model!\n","3. **The Optimizer** - this part is the bit that actually changes the model parameters. It has a sense for the direction we should be shooting and updates all of the internal numbers inside the model to find the best internal knobs to predict the right digits. In this case I am using the Binary Cross Entropy cost function because, well, I know it works. There are a ton of different cost functions you can choose from that fit a variety of different scenarios."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# where to run\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print('Using {} device'.format(device))\n","\n","# selected model - you can uncomment\n","# the other models for future runs\n","model = SimpleLinear().to(device)\n","#model = NeuralNework().to(device)\n","#model = CNN().to(device)\n","print(model)\n","\n","# cost function used to determine best parameters\n","cost = torch.nn.BCELoss()\n","\n","# used to create optimal parameters\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"]},{"cell_type":"markdown","metadata":{},"source":["I forgot to mention the whole `cuda` thing. GPUs speed this whole process up because this is basically all a big Matrix multiplcation problem in a `for` loop. PyTorch is great because you basically need to tell the model where to run (either on the CPU or using CUDA - which is a platform for moving computations to the GPU).\n","\n","Now for the learning part! The `dataloader`'s job is to iterate through the entire dataset (in this case 60,000 examples of digits and their corresponding label) but to take chunks of size `batch_size` of the data to process. This is another hyperparameter that needs to be chosen. `epochs` is the number of times we want to loop through the dataset in its entirety (again something we choose based upon how our experiment goes).\n","\n","A word on how long this takes - it takes a while."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# loader\n","dataloader = DataLoader(digits, batch_size=batch_size, num_workers=0, pin_memory=True)\n","\n","# golfing time!\n","for t in range(epochs):\n","    print('Epoch {}'.format(t))\n","    print('-------------------------------')\n","    # go through the entire dataset once\n","    for batch, (X, Y) in enumerate(dataloader):\n","        X, Y = X.to(device), Y.to(device)\n","        # zero out gradient\n","        optimizer.zero_grad()\n","        # make a prediction on this batch!\n","        pred = model(X)\n","        # how bad is it?\n","        loss = cost(pred, Y)\n","        # compute gradients\n","        loss.backward()\n","        # update parameters\n","        optimizer.step()\n","        \n","        if batch % 100 == 0:\n","            print('loss: {:>10f}  [{:>5d}/{:>5d}]'.format(loss.item(), batch * len(X), len(dataloader.dataset)))\n","            \n","    print('loss: {:>10f}  [{:>5d}/{:>5d}]\\n'.format(loss.item(), len(dataloader.dataset), len(dataloader.dataset)))\n","    \n","print('Done!')"]},{"cell_type":"markdown","metadata":{},"source":["So what the heck did this actually do? Great question. I will explain this `SimpleLinear` model since its the on we ran first. We are basically learning one matrix and one vector. That's it (it's really anti-climacttic if you ask me). Let me show you why that actually does anything:\n","\n","\\begin{align}\n","prediction = W \\cdot x + b\n","\\end{align}\n","\n","\\begin{align}\n","\\begin{bmatrix}\n","\\hat{y}_1 \\\\ \\vdots \\\\ \\hat{y}_{10}\n","\\end{bmatrix}_{10 \\times 1} = \n","\\begin{bmatrix}\n","w_{1,1} &  \\ldots &  w_{1,784} \\\\\n","\\vdots    & \\ddots &  \\\\\n","w_{10,1}  & \\ldots & w_{10, 784} \\\\\n","\\end{bmatrix}_{10 \\times 784} \n","\\cdot\n","\\begin{bmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_{783} \\\\ x_{784}\\end{bmatrix}_{784 \\times 1}\n"," + \n","\\begin{bmatrix}\n","b_1 \\\\ \\vdots \\\\ b_{10}\n","\\end{bmatrix}_{10 \\times 1}\n","\\end{align}\n"]},{"cell_type":"markdown","metadata":{},"source":["The matrix `W`and the vector `b` is what the `SimpleLinear` model learns. The output is a 10 by 1 matrix whose largest value is the index of the thing number we want to predict. Take a look at what the algorithm learned for the two variables (as well as the sizes):"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for p in model.parameters():\n","    print(p.shape)\n","    print(p)"]},{"cell_type":"markdown","metadata":{},"source":["# Is it working???\n","The best way to figure out if it is working is to test the model on data the learning process hasn't used. Luckily we have such a dataset (it is basically a held out section of the data we already have used). I'm loading it all up the same way as before and printing them out to show you that they're different."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_digits = datasets.MNIST('data', train=False, download=True,\n","                        transform=transforms.Compose([\n","                            transforms.ToTensor(),\n","                            transforms.Lambda(lambda x: x.reshape(28*28))\n","                        ]),\n","                        target_transform=transforms.Compose([\n","                            transforms.Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1))\n","                        ])\n","                     )\n","draw_digits(test_digits)"]},{"cell_type":"markdown","metadata":{},"source":["Let's test it on the first digit above (the 7)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x, y = test_digits[0]\n","x = x.to(device).view(1, 28*28)"]},{"cell_type":"markdown","metadata":{},"source":["We'll tell the model we are using it for evaluation (sometimes called inference) and pass in our previously unseen digit."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.eval()\n","with torch.no_grad():\n","    pred = model(x)\n","    pred = pred.to('cpu').detach()[0]\n","    \n","print(pred)"]},{"cell_type":"markdown","metadata":{},"source":["Now let's see if the predicted digit matches the actual digit:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred.argmax(0), y.argmax(0)"]},{"cell_type":"markdown","metadata":{},"source":["Let's see how well we do over *all* of the test data!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# data loader for test digits\n","test_dataloader = DataLoader(test_digits, batch_size=64, num_workers=0, pin_memory=True)\n","\n","# set model to evaluation mode\n","model.eval()\n","test_loss = 0\n","correct = 0\n","\n","# loop! \n","with torch.no_grad():\n","    for batch, (X, Y) in enumerate(dataloader):\n","        X, Y = X.to(device), Y.to(device)\n","        pred = model(X)\n","\n","        test_loss += cost(pred, Y).item()\n","        correct += (pred.argmax(1) == Y.argmax(1)).type(torch.float).sum().item()\n","\n","test_loss /= len(dataloader.dataset)\n","correct /= len(dataloader.dataset)\n","print('\\nTest Error:')\n","print('acc: {:>0.1f}%, avg loss: {:>8f}'.format(100*correct, test_loss))"]},{"cell_type":"markdown","metadata":{},"source":["# Saving the Model\n","Every framework is different - in this case PyTorch let's us save the model (which you remember is just a big matrix `W` and a vector `b`) to an internal format as well as to the ONNX format. These can then be loaded up as an asset to a program that is executed every time you need to recognize a digit!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# create dummy variable to traverse graph\n","x = torch.randint(255, (1, 28*28), dtype=torch.float).to(device) / 255\n","onnx.export(model, x, 'model.onnx')\n","print('Saved onnx model to model.onnx')\n","\n","# saving PyTorch Model Dictionary\n","torch.save(model.state_dict(), 'model.pth')\n","print('Saved PyTorch Model to model.pth')"]},{"cell_type":"markdown","metadata":{},"source":["# Play!\n","Now that you've gone through the whole process, please go back up to play around! Try changing:\n","1. The actual model! The other models are almost identical with the exception that they learn additional Matrices (W's and b's) that the images pass through to get the final answer.\n","2. The hyperparameters like `learning_rate`, `batch_size`, and `epoch`. Does it make things better or worse? 92% is ok, does any other combination of model's and hyperparamters fare better?\n","\n","## Final Thoughts\n","Would love your feedback! Was this helpful? Any parts confusing? Drop me a line!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.10-final"},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3,"kernelspec":{"name":"Python 3.6.10 64-bit ('torch': conda)","display_name":"Python 3.6.10 64-bit ('torch': conda)","metadata":{"interpreter":{"hash":"d412cf0cc95275da7f3e5a2cde727869a6beaae6991f4cbb39a087a6b0868edc"}}}}}